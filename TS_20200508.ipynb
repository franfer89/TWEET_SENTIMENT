{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEET SENTIMENT EXTRACTION\n",
    "\n",
    "**Kaggle competition**: [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)\n",
    "\n",
    "Several tweets are classified as neutral, positive or negative. The task consist in extracting which part of the tweet makes the tweet neutral, positive or negative.\n",
    "\n",
    "**Example**\n",
    "\n",
    "INPUT:\n",
    "\n",
    "tweet sentence:\"  SWEEEEET - San Fran is awesome!!!!  Love it there \"\n",
    "\n",
    "*it is known to be defined as positive*\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "it is positive because it contains: \" Love it there \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "**train.csv** contains 4 columns:\n",
    "1.  Tweet ID\n",
    "2.  text: tweet sentence\n",
    "3.  selected_text: part of the tweet which defines the sentiment\n",
    "4.  sentiment: positive, neutral or negative\n",
    "\n",
    "**test.csv** contains 3 columns:\n",
    "1.  Tweet ID\n",
    "2.  text: tweet sentence\n",
    "3.  sentiment: positive, neutral or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA=\"Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_extraction(file):\n",
    "    ## extraction of data from files\n",
    "    corpus = []\n",
    "    with open(file) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            corpus.append(row)\n",
    "    return corpus\n",
    "\n",
    "def corpus_selection(corpus,selection,selection_column):\n",
    "    ## This function extract a sub-corpus containing only one sentiment type\n",
    "    ## INPUTS\n",
    "    #  corpus\n",
    "    #  selection: \"postivive\", \"neutral\", \"negative\"\n",
    "    #  selection_column: contains the column of corpus in which sentiment is found\n",
    "    ## OUTPUT\n",
    "    #  corpus_s: sub-corpus\n",
    "    corpus_s=[]\n",
    "    for row in corpus:\n",
    "        if row[selection_column]==selection:\n",
    "            corpus_s.append(row)\n",
    "    return corpus_s\n",
    "\n",
    "def separate(corpus):\n",
    "    ## Extract sentence (tweet), index (ID) and sentence_label (selected part of the tweet) from a corpus\n",
    "    sentence=[]\n",
    "    index=[]\n",
    "    sentence_label=[]\n",
    "    for row in corpus:\n",
    "        sentence.append(row[1])\n",
    "        index.append(row[0])\n",
    "        sentence_label.append(row[2])\n",
    "    return sentence,index,sentence_label\n",
    "\n",
    "def separate_test(corpus):\n",
    "    ## Extract sentence (tweet) and index (ID) from a corpus\n",
    "    sentence=[]\n",
    "    index=[]\n",
    "    for row in corpus:\n",
    "        sentence.append(row[1])\n",
    "        index.append(row[0])\n",
    "    return sentence,index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus creaetion\n",
    "corpus=corpus_extraction(PATH_DATA+\"train.csv\")\n",
    "corpus_test=corpus_extraction(PATH_DATA+\"test.csv\")\n",
    "\n",
    "## TRAINING DATA\n",
    "## Corpus separation according to sentiment\n",
    "corpus_neutral=corpus_selection(corpus,selection='neutral',selection_column=3)\n",
    "corpus_positive=corpus_selection(corpus,selection='positive',selection_column=3)\n",
    "corpus_negative=corpus_selection(corpus,selection='negative',selection_column=3)\n",
    "\n",
    "## Extraction of sentence\n",
    "sentence_neutral,index_neutral,sentence_label_neutral=separate(corpus_neutral)\n",
    "sentence_positive,index_positive,sentence_label_positive=separate(corpus_positive)\n",
    "sentence_negative,index_negative,sentence_label_negative=separate(corpus_negative)\n",
    "\n",
    "number_positive=len(sentence_positive)\n",
    "number_negative=len(sentence_negative)\n",
    "\n",
    "# positive and negative tweets shall be trained together\n",
    "sentence=sentence_positive+sentence_negative\n",
    "index=index_positive+index_negative\n",
    "sentence_label=sentence_label_positive+sentence_label_negative\n",
    "\n",
    "\n",
    "## TEST DATA\n",
    "corpus_test_neutral=corpus_selection(corpus_test,selection='neutral',selection_column=2)\n",
    "corpus_test_positive=corpus_selection(corpus_test,selection='positive',selection_column=2)\n",
    "corpus_test_negative=corpus_selection(corpus_test,selection='negative',selection_column=2)\n",
    "\n",
    "sentence_test_neutral,index_test_neutral=separate_test(corpus_test_neutral)\n",
    "sentence_test_positive,index_test_positive=separate_test(corpus_test_positive)\n",
    "sentence_test_negative,index_test_negative=separate_test(corpus_test_negative)\n",
    "\n",
    "number_test_positive=len(sentence_test_positive)\n",
    "number_test_negative=len(sentence_test_negative)\n",
    "\n",
    "sentence_test=sentence_test_positive+sentence_test_negative\n",
    "index_test=index_test_positive+index_test_negative\n",
    "\n",
    "del sentence_positive,sentence_test_positive\n",
    "del sentence_negative,sentence_test_negative\n",
    "del index_positive,index_test_positive\n",
    "del index_negative,index_test_negative\n",
    "del sentence_label_positive\n",
    "del sentence_label_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORE: word-level Jaccard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences positive and negative:  16363\n",
      "initial prediction score for positive and negative:  0.32589890326672905\n",
      "\n",
      "\n",
      "Number of sentences neutral:  11118\n",
      "initial prediction score for neutral:  0.9763568439593936\n",
      "\n",
      "\n",
      "Expected initial score:  0.5890549523414005\n"
     ]
    }
   ],
   "source": [
    "def jaccard(str1, str2): \n",
    "    ## JACCARD score between two strings\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    if np.abs(len(a) + len(b) - len(c))>0:\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def all_jaccard(sentence_label,sentence_label_pred):\n",
    "    ## Jaccard average score between a list of sentence_label and list of sentence_label predicted\n",
    "    cont=0\n",
    "    for i in range(len(sentence_label)):\n",
    "        #print(i)\n",
    "        #print(sentence_label[i],\"/////\",sentence_label_pred[i])\n",
    "        cont=cont+jaccard(sentence_label[i], sentence_label_pred[i])\n",
    "    cont=cont/len(sentence_label)\n",
    "    return cont\n",
    "\n",
    "n_pn=len(sentence)\n",
    "score_pn=all_jaccard(sentence_label,sentence)\n",
    "n_n=len(sentence_neutral)\n",
    "score_n=all_jaccard(sentence_label_neutral,sentence_neutral)\n",
    "\n",
    "print(\"Number of sentences positive and negative: \",n_pn)\n",
    "print(\"initial prediction score for positive and negative: \",score_pn)\n",
    "print(\"\\n\")\n",
    "print(\"Number of sentences neutral: \",n_n)\n",
    "print(\"initial prediction score for neutral: \",score_n)\n",
    "print(\"\\n\")\n",
    "print(\"Expected initial score: \",((n_pn*score_pn+n_n*score_n)/(n_pn+n_n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that simply by assuming sentence_label is equal to sentence, the initial score is 0.589. The maximum public score at present time obtained in the kaggle competition is 0.731. There is little margin for improvement (About 0.14)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_filter(sentence):\n",
    "    ## This function adds \"/\" around any sign (of the ones contained in string sign_included) found in the sentence.\n",
    "    #  sentence can be a list of sentences or a single sentence.\n",
    "    sign_included='!\"#$%&()+,-;<=>?@^_`{|}~'\n",
    "    islist=0\n",
    "    if isinstance(sentence, str):\n",
    "        sentence=[sentence]\n",
    "        islist=1\n",
    "    sentence=sentence.copy()\n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(len(sign_included)):\n",
    "            sentence[i]=sentence[i].replace(sign_included[j],\"/\"+sign_included[j]+\"/\")\n",
    "    if islist==1:\n",
    "        return sentence[0]\n",
    "    if islist==0:\n",
    "        return sentence\n",
    "\n",
    "def to_sequence(sentence,tokenizer):\n",
    "    ## This function tokenizes the sentece (it can be a list of sentences or a single sentence)\n",
    "    sentence=separate_filter(sentence)\n",
    "    if isinstance(sentence, str):\n",
    "        sentence=[sentence]\n",
    "    sequence=tokenizer.texts_to_sequences(sentence)\n",
    "    return sequence\n",
    "\n",
    "def to_sentence(sequence):\n",
    "    ## This function converts from sequence to sentence\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in sequence])\n",
    "\n",
    "def create_model_input_pn(sentence,tokenizer,number_positive,number_negative,length=-1):\n",
    "    ## This function creates the model input for positive and negative sentences\n",
    "    sequence=to_sequence(sentence,tokenizer)\n",
    "    if length==-1:\n",
    "        sequence = pad_sequences(sequence,padding='post',truncating='post')\n",
    "        length=len(sequence[0])\n",
    "    else:\n",
    "        sequence = pad_sequences(sequence,maxlen=length,padding='post',truncating='post')\n",
    "    sentiment=np.vstack((np.zeros((number_positive,length))+word_index['positive'],np.zeros((number_negative,length))+word_index['negative']))\n",
    "    sequence=np.array(sequence).reshape(-1,length,1)  \n",
    "    \n",
    "    x=np.concatenate([sequence,sentiment.reshape(-1,length,1)],axis=2)\n",
    "    return x\n",
    "\n",
    "def create_model_input_n(sentence,tokenizer,length=-1):\n",
    "    ## This function creates the model input for neutral sentences\n",
    "    sequence=to_sequence(sentence,tokenizer)\n",
    "    if length==-1:\n",
    "        sequence = pad_sequences(sequence,padding='post',truncating='post')\n",
    "        length=len(sequence[0])\n",
    "    else:\n",
    "        sequence = pad_sequences(sequence,maxlen=length,padding='post',truncating='post')\n",
    "    sentiment=np.zeros((len(sentence),length))+word_index['neutral']\n",
    "    sequence=np.array(sequence).reshape(-1,length,1)\n",
    "    x=np.concatenate([sequence,sentiment.reshape(-1,length,1)],axis=2)\n",
    "    return x\n",
    "\n",
    "def print_example(sentence,sequence,sentence_label,example_number):\n",
    "    print(\"sentence:\\\"\", sentence[example_number],\"\\\"\")\n",
    "    print(\"sequence: \",sequence[example_number])\n",
    "    print(\"sentence label:\\\"\", sentence_label[example_number],\"\\\"\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer creation\n",
    "FILTERS='./:[\\\\]\\t\\n'\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\",filters=FILTERS)\n",
    "tokenizer.fit_on_texts(separate_filter(sentence+sentence_test+sentence_neutral+[\"neutral\"]))\n",
    "\n",
    "# word_index contains a dictionary with the index of each token\n",
    "word_index = tokenizer.word_index\n",
    "# reverse_word_index contains a dictionary with the token of each index\n",
    "reverse_word_index=dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "# Number of tokens\n",
    "vocab_size=len(word_index)\n",
    "\n",
    "length=103\n",
    "#initial_length=63\n",
    "# Inputs to model\n",
    "x=create_model_input_pn(sentence,tokenizer,number_positive,number_negative,length=length)\n",
    "x_test=create_model_input_pn(sentence_test,tokenizer,number_test_positive,number_test_negative,length=length)\n",
    "\n",
    "# Inputs to model\n",
    "x_neutral=create_model_input_n(sentence_neutral,tokenizer,length=length)\n",
    "x_test_neutral=create_model_input_n(sentence_test_neutral,tokenizer,length=length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_label(sentence,sentence_label,tokenizer):\n",
    "    ## This function returns the token number at which the sentence_label starts in the sentence\n",
    "    sentence_label=separate_filter(sentence_label)\n",
    "    sentence=separate_filter(sentence)\n",
    "    index = sentence.find(sentence_label)\n",
    "    sequence=tokenizer.texts_to_sequences([sentence[:index]])\n",
    "    return len(sequence[0])\n",
    "\n",
    "def end_label(sentence,sentence_label,tokenizer):\n",
    "    ## This function returns the token number at which the sentence_label ends in the sentence\n",
    "    sentence_label=separate_filter(sentence_label)\n",
    "    sentence=separate_filter(sentence)\n",
    "    index = sentence.find(sentence_label)\n",
    "    sequence=tokenizer.texts_to_sequences([sentence[:index+len(sentence_label)+1]])\n",
    "    return len(sequence[0])-1\n",
    "\n",
    "def y_label(sentence,sentence_label,tokenizer,length):\n",
    "    ## This function returns a list of start_label and end_label for each sentence.\n",
    "    y_start=[]\n",
    "    y_end=[]\n",
    "    y=np.zeros((len(sentence),length))\n",
    "    for i in range(len(sentence)):\n",
    "        y_start.append(start_label(sentence[i],sentence_label[i],tokenizer))\n",
    "        y_end.append(end_label(sentence[i],sentence_label[i],tokenizer))\n",
    "        y[i,start_label(sentence[i],sentence_label[i],tokenizer):end_label(sentence[i],sentence_label[i],tokenizer)]=y[i,start_label(sentence[i],sentence_label[i],tokenizer):end_label(sentence[i],sentence_label[i],tokenizer)]+1\n",
    "    return y_start,y_end,y\n",
    "\n",
    "\n",
    "y_start,y_end,y=y_label(sentence,sentence_label,tokenizer,length)\n",
    "y_start_neutral,y_end_neutral,y_neutral=y_label(sentence_neutral,sentence_label_neutral,tokenizer,length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2text(sentence,y,tokenizer):\n",
    "    # This function creates a sentence_label using the sentence and the y labels.\n",
    "    n1=len(sentence)\n",
    "    nq1=len(to_sequence(sentence,tokenizer)[0])\n",
    "    sentence=[sentence].copy()\n",
    "    sentence=sentence[0]\n",
    "    if y[0]>=nq1:\n",
    "        y[0]=nq1-1\n",
    "    if y[1]<=y[0]:\n",
    "        y[1]=y[0]\n",
    "    if y[1]>=nq1:\n",
    "        y[1]=y[0]\n",
    "    if y[0]>0:\n",
    "        cont=0\n",
    "        while cont<n1:\n",
    "            seq_aux=to_sequence(sentence[cont:],tokenizer)\n",
    "            if len(seq_aux[0])==nq1-y[0]:\n",
    "                sentence=sentence[cont:]\n",
    "                break\n",
    "            cont=cont+1\n",
    "        n1=len(sentence)\n",
    "    cont=n1\n",
    "    while cont>0:\n",
    "        seq_aux=to_sequence(sentence[:cont],tokenizer)\n",
    "        if len(seq_aux[0])==y[1]-y[0]+1:\n",
    "            sentence=sentence[:cont]\n",
    "            break\n",
    "        cont=cont-1\n",
    "    while (FILTERS+\" \").find(sentence[0])!=-1:\n",
    "        sentence=sentence[1:]\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {};\n",
    "embedding_dim=100\n",
    "with open('glove.6B.100d.txt',encoding='UTF-8') as f:\n",
    "#with open('glove.42B.300d.txt',encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbatchrelu(model,f,k,drop_prob=0):\n",
    "    ## INPUTS\n",
    "    # model\n",
    "    # f: number filters\n",
    "    # k: kernel size\n",
    "    # drop_prob: drop-out probability (if 0, dropout removed)\n",
    "    ## OUTPUT\n",
    "    # model\n",
    "    \n",
    "    model = tf.keras.layers.Conv1D(f,k,padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.ReLU()(model)\n",
    "    if drop_prob>0:\n",
    "        model = tf.keras.layers.Dropout(drop_prob)(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(seed):\n",
    "    ## CREATES A MODEL\n",
    "    ## seed: random seed for the model\n",
    "    \n",
    "    tf.random.set_seed(seed)\n",
    "    drop_prob=0.5  # Drop-our rate\n",
    "    lambda_reg=0   # L2 regularization lambda\n",
    "\n",
    "    ## INPUT layer\n",
    "    inputs = tf.keras.layers.Input(shape = (int(length),2))\n",
    "\n",
    "    #mask = tf.keras.layers.Reshape((length,1))(inputs[:,:,2])\n",
    "\n",
    "    \n",
    "    ## Embedding\n",
    "    model = tf.keras.layers.Embedding(vocab_size+1, embedding_dim, weights=[embeddings_matrix], trainable=True)(inputs[:,:,0])\n",
    "    \n",
    "    ## model multiplied by the embedding of special word: \"psotive\" or \"negative\" or \"neutral\"\n",
    "    positive=tf.keras.layers.Embedding(vocab_size+1, embedding_dim, weights=[embeddings_matrix], trainable=True)(inputs[:,:,1])\n",
    "    model = tf.keras.layers.Multiply()([model, positive])\n",
    "\n",
    "    model = tf.keras.layers.Dropout(drop_prob)(model)\n",
    "    out1=model\n",
    "    model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,return_sequences=True,kernel_regularizer=regularizers.l2(lambda_reg)))(model)\n",
    "    model = tf.keras.layers.Dense(embedding_dim,kernel_regularizer=regularizers.l2(lambda_reg))(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.ReLU()(model)\n",
    "    model = tf.keras.layers.Dropout(drop_prob)(model)\n",
    "    model = tf.keras.layers.Add()([model,out1])\n",
    "\n",
    "    out2=model\n",
    "    model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,return_sequences=True,kernel_regularizer=regularizers.l2(lambda_reg)))(model)\n",
    "    model = tf.keras.layers.Dense(embedding_dim,kernel_regularizer=regularizers.l2(lambda_reg))(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.ReLU()(model)\n",
    "    model = tf.keras.layers.Dropout(drop_prob)(model)\n",
    "\n",
    "    model = tf.keras.layers.Add()([model,out1,out2])\n",
    "    \n",
    "\n",
    "    output_start = tf.keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=regularizers.l2(lambda_reg))(model)\n",
    "    output_end = tf.keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=regularizers.l2(lambda_reg))(model)\n",
    "    output_start = tf.keras.layers.Flatten(name='start')(output_start)\n",
    "    output_end = tf.keras.layers.Flatten(name='end')(output_end)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[output_start,output_end])\n",
    "    #model.summary()\n",
    "\n",
    "    ## Compilation\n",
    "    model.compile(optimizer = tf.optimizers.Adam(),\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#create_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(checkpoint_filepath):\n",
    "    ## Creation of callbacks\n",
    "    ## INPUT\n",
    "    #  checkpoint_filepath: path for checkpoint\n",
    "    \n",
    "    ## REDUCTION LEARNING RATE ON PLATEAU\n",
    "    learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_start_accuracy', \n",
    "                                                                   patience=4, \n",
    "                                                                   verbose=1, \n",
    "                                                                   factor=0.5, \n",
    "                                                                   min_lr=0.00000001)\n",
    "\n",
    "    ## EARLY STOPPING\n",
    "    callback_earlyS = tf.keras.callbacks.EarlyStopping(monitor='val_start_accuracy', patience=7) \n",
    "\n",
    "    ## Nice plotting of progressbar\n",
    "    #tqdm_callbacks = tfa.callbacks.TQDMProgressBar()\n",
    "\n",
    "    ## Checkpoint everytime accuracy is maximum\n",
    "    #checkpoint_filepath = '/tmp/checkpoint'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath=checkpoint_filepath,\n",
    "                                save_weights_only=True,\n",
    "                                monitor='val_start_accuracy',\n",
    "                                mode='max',\n",
    "                                save_best_only=True)\n",
    "    \n",
    "    return [learning_rate_reduction,callback_earlyS,model_checkpoint_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,x_train,y_start_train,y_end_train,x_CV,y_start_CV,y_end_CV):\n",
    "    checkpoint_filepath = '/tmp/checkpoint'\n",
    "    CALLBACKS=create_callbacks(checkpoint_filepath)\n",
    "    \n",
    "    batchsize=128\n",
    "    history=model.fit(x_train,[y_start_train,y_end_train],\n",
    "                     validation_data = (x_CV, [y_start_CV,y_end_CV]),\n",
    "                     batch_size=batchsize,\n",
    "                     #steps_per_epoch=x_train.shape[0] // batchsize,\n",
    "                     epochs=50,\n",
    "                     verbose=1,\n",
    "                     callbacks=CALLBACKS)\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_model(model,x,sentence,sentence_label,tokenizer):\n",
    "    # calculate Jaccard score on cross-validation data\n",
    "    def evaluate_all_jaccard(sentence,sentence_label,y,tokenizer):\n",
    "        cont=0\n",
    "        for i in range(len(sentence)):\n",
    "            sentence_label_predicted=y2text(sentence[i],y[i],tokenizer)\n",
    "            cont=cont+jaccard(sentence_label[i], sentence_label_predicted)\n",
    "            #print(sentence_label_predicted,\"/////\",sentence_label[i])\n",
    "        cont=cont/len(sentence)\n",
    "        return cont\n",
    "    Y_pred=np.array(model.predict(x))\n",
    "    y_pred=Y_pred.argmax(axis=2).transpose()\n",
    "    score=evaluate_all_jaccard(sentence,sentence_label,y_pred,tokenizer)\n",
    "    print(\"Score:\", score)\n",
    "    return score\n",
    "    \n",
    "def test_model(listmodel,x_test,sentence_test,tokenizer):\n",
    "    # creates the sentence_label for test data\n",
    "\n",
    "    def sentence_label_function(sentence,y,tokenizer):\n",
    "        sentence_label=[]\n",
    "        for i in range(len(sentence)):\n",
    "            if y[i][1]<y[i][0]:\n",
    "                y[i][1]=y[i][0]\n",
    "            sentence_label_predicted=y2text(sentence[i],y[i],tokenizer)\n",
    "            sentence_label.append(sentence_label_predicted)\n",
    "        return sentence_label\n",
    "    \n",
    "    Y_pred_list=[]\n",
    "    for model in listmodel:\n",
    "        Y_pred_list.append(model.predict(x_test))\n",
    "    Y_pred=Y_pred_list[0]\n",
    "    if len(Y_pred_list)>0:\n",
    "        for i in range(1,len(Y_pred_list)):\n",
    "            Y_pred=Y_pred+Y_pred_list[i]\n",
    "    y_pred=np.array(Y_pred).argmax(axis=2).transpose()  \n",
    "    sentence_label_test=sentence_label_function(sentence_test,y_pred,tokenizer)\n",
    "    return sentence_label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.vstack([x,x_neutral])\n",
    "X_test=np.vstack([x_test,x_test_neutral])\n",
    "\n",
    "SENTENCE=sentence+sentence_neutral\n",
    "SENTENCE_LABEL=sentence_label+sentence_label_neutral\n",
    "SENTENCE_TEST=sentence_test+sentence_test_neutral\n",
    "\n",
    "Y_start=y_start+y_start_neutral\n",
    "Y_end=y_end+y_end_neutral\n",
    "\n",
    "classes=X[:,0,1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/50\n",
      "21984/21984 [==============================] - 17s 789us/sample - loss: 4.9341 - start_loss: 2.1658 - end_loss: 2.7667 - start_accuracy: 0.5050 - end_accuracy: 0.0781 - val_loss: 3.9108 - val_start_loss: 1.7205 - val_end_loss: 2.1874 - val_start_accuracy: 0.5741 - val_end_accuracy: 0.2920\n",
      "Epoch 2/50\n",
      " 3712/21984 [====>.........................] - ETA: 10s - loss: 3.6615 - start_loss: 1.6750 - end_loss: 1.9865 - start_accuracy: 0.5745 - end_accuracy: 0.3418WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_start_accuracy` which is not available. Available metrics are: loss,start_loss,end_loss,start_accuracy,end_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_start_accuracy` which is not available. Available metrics are: loss,start_loss,end_loss,start_accuracy,end_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_start_accuracy available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-d189498838d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_start_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_end_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_CV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_start_CV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_end_CV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmodel_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-157-21bd542a07d2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, x_train, y_start_train, y_end_train, x_CV, y_start_CV, y_end_CV)\u001b[0m\n\u001b[0;32m     10\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                      callbacks=CALLBACKS)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "score=[]\n",
    "for kfold,(train,CV) in enumerate(skf.split(X,classes)):\n",
    "    x_train=X[train,:,:]\n",
    "    x_CV=X[CV,:,:]\n",
    "    y_start_train=tf.one_hot(Y_start,length).numpy()[train,:]\n",
    "    y_end_train=tf.one_hot(Y_end,length).numpy()[train,:]\n",
    "    y_start_CV=tf.one_hot(Y_start,length).numpy()[CV,:]\n",
    "    y_end_CV=tf.one_hot(Y_end,length).numpy()[CV,:]\n",
    "    sen_CV=[]\n",
    "    sen_label_CV=[]\n",
    "    for i in CV:\n",
    "        sen_CV.append(SENTENCE[i])\n",
    "        sen_label_CV.append(SENTENCE_LABEL[i])\n",
    "        \n",
    "    model_list=[]\n",
    "    model=create_model(kfold)\n",
    "    model=train_model(model,x_train,y_start_train,y_end_train,x_CV,y_start_CV,y_end_CV)\n",
    "    model_list.append(model)\n",
    "    \n",
    "    score.append(CV_model(model,x_CV,sen_CV,sen_label_CV,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(model.predict(x_CV)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and save submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_label_test=test_model(model_list,X_test,SENTENCE_TEST,tokenizer)  \n",
    "\n",
    "index_test_REC=index_test+index_test_neutral\n",
    "sentence_label_test_REC=sentence_label_test#+sentence_test_neutral\n",
    "\n",
    "REC= pd.DataFrame()\n",
    "REC['textID']=index_test_REC\n",
    "REC['selected_text']=sentence_label_test_REC\n",
    "REC[['textID','selected_text']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
